#!/usr/bin/env python3
"""
OpenVLA-7B æµ‹è¯•è„šæœ¬

ç”¨äºéªŒè¯ OpenVLA æ¨¡å‹æ˜¯å¦æ­£ç¡®åŠ è½½å’Œè¿è¡Œ
ä¸éœ€è¦ ROS æˆ–æœºå™¨äººç¡¬ä»¶ï¼Œä»…æµ‹è¯•æ¨¡å‹æœ¬èº«
"""

import os
import sys
import torch
import numpy as np
from PIL import Image

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def test_model_loading():
    """æµ‹è¯•æ¨¡å‹åŠ è½½"""
    print("=" * 60)
    print("æµ‹è¯• 1: æ¨¡å‹åŠ è½½")
    print("=" * 60)
    
    try:
        from eval_real_franka import openvla_act_policy
        
        policy_config = {
            "policy_type": "openvla",
            "model_path": "~/Desktop/openvla/openvla-7b",
            "action_dim": 7,
            "chunk_size": 50,
        }
        
        print("æ­£åœ¨åŠ è½½ OpenVLA æ¨¡å‹...")
        policy = openvla_act_policy(policy_config)
        
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        print(f"   è®¾å¤‡: {next(policy.policy.parameters()).device}")
        print(f"   æ•°æ®ç±»å‹: {next(policy.policy.parameters()).dtype}")
        print(f"   åŠ¨ä½œç»´åº¦: {policy.config.action_dim}")
        print(f"   åºåˆ—é•¿åº¦: {policy.config.chunk_size}")
        
        return policy
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def test_image_processing(policy):
    """æµ‹è¯•å›¾åƒå¤„ç†"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 2: å›¾åƒå¤„ç†")
    print("=" * 60)
    
    try:
        # åˆ›å»ºæ¨¡æ‹Ÿå›¾åƒ (2 ä¸ªç›¸æœºï¼Œ640x480, RGB)
        left_image = np.random.randint(0, 256, (480, 640, 3), dtype=np.uint8)
        right_image = np.random.randint(0, 256, (480, 640, 3), dtype=np.uint8)
        
        print(f"åˆ›å»ºæ¨¡æ‹Ÿå›¾åƒ:")
        print(f"  å·¦ç›¸æœº: {left_image.shape}")
        print(f"  å³ç›¸æœº: {right_image.shape}")
        
        # è½¬æ¢ä¸º torch tensor (å½’ä¸€åŒ–åˆ° [0, 1])
        images = np.stack([left_image, right_image], axis=0)
        curr_image = torch.from_numpy(images / 255.0).float().cuda()
        
        print(f"è½¬æ¢ä¸º tensor: {curr_image.shape}")
        
        # åˆ›å»ºæ¨¡æ‹Ÿæœºå™¨äººçŠ¶æ€ (7 DOF)
        robot_state = torch.randn(1, 7).float().cuda()
        print(f"æœºå™¨äººçŠ¶æ€: {robot_state.shape}")
        
        # ä»»åŠ¡æè¿°
        task_description = "pick up the white bowl"
        print(f"ä»»åŠ¡æè¿°: {task_description}")
        
        # å¤„ç†è¾“å…¥
        print("\næ­£åœ¨å¤„ç†è¾“å…¥æ‰¹æ¬¡...")
        batch = policy.process_batch_to_llava(
            curr_image, robot_state, task_description
        )
        
        print("âœ… å›¾åƒå¤„ç†æˆåŠŸ")
        print(f"   æ‰¹æ¬¡ keys: {batch.keys()}")
        print(f"   å›¾åƒå½¢çŠ¶: {batch['images'].shape}")
        print(f"   æç¤ºè¯: {batch['prompt'][:80]}...")
        print(f"   çŠ¶æ€å½¢çŠ¶: {batch['states'].shape}")
        
        return batch
        
    except Exception as e:
        print(f"âŒ å›¾åƒå¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def test_inference(policy, batch):
    """æµ‹è¯•æ¨ç†"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 3: æ¨¡å‹æ¨ç†")
    print("=" * 60)
    
    try:
        print("æ­£åœ¨è¿è¡Œæ¨ç†...")
        
        with torch.inference_mode():
            # ä½¿ç”¨ processor å¤„ç†è¾“å…¥
            # å°†å›¾åƒè½¬æ¢å› numpy æ ¼å¼ (HWC)
            image_np = batch['images'].cpu().numpy().transpose(0, 2, 3, 1)
            
            inputs = policy.processor(
                text=batch['prompt'],
                images=image_np,
                return_tensors="pt"
            ).to("cuda")
            
            print(f"Processor è¾“å…¥ keys: {inputs.keys()}")
            
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æœ‰ predict_action æ–¹æ³•
            if hasattr(policy.policy, 'predict_action'):
                print("ä½¿ç”¨ predict_action æ–¹æ³•...")
                actions = policy.policy.predict_action(
                    **inputs, 
                    unnorm_key="bridge_orig"
                )
            else:
                print("ä½¿ç”¨æ ‡å‡† forward æ–¹æ³•...")
                actions = policy.policy(**inputs)
        
        print("âœ… æ¨ç†æˆåŠŸ")
        print(f"   è¾“å‡ºå½¢çŠ¶: {actions.shape}")
        print(f"   è¾“å‡ºèŒƒå›´: [{actions.min():.3f}, {actions.max():.3f}]")
        print(f"   è¾“å‡ºæ ·æœ¬ (å‰ 5 ä¸ªå€¼): {actions.flatten()[:5].cpu().numpy()}")
        
        # éªŒè¯è¾“å‡ºç»´åº¦
        if actions.dim() == 3:
            batch_size, horizon, action_dim = actions.shape
            print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
            print(f"   æ—¶é—´æ­¥é•¿: {horizon}")
            print(f"   åŠ¨ä½œç»´åº¦: {action_dim}")
        elif actions.dim() == 2:
            horizon, action_dim = actions.shape
            print(f"   æ—¶é—´æ­¥é•¿: {horizon}")
            print(f"   åŠ¨ä½œç»´åº¦: {action_dim}")
        
        return actions
        
    except Exception as e:
        print(f"âŒ æ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def test_config():
    """æµ‹è¯•é…ç½®æ–‡ä»¶"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 4: é…ç½®æ–‡ä»¶")
    print("=" * 60)
    
    try:
        from openvla_config import get_openvla_config, OPENVLA_SPECS
        
        config = get_openvla_config()
        
        print("âœ… é…ç½®åŠ è½½æˆåŠŸ")
        print("\nOpenVLA é…ç½®:")
        for key, value in config.items():
            print(f"  {key}: {value}")
        
        print("\nOpenVLA ç‰¹æ€§:")
        for key, value in OPENVLA_SPECS.items():
            print(f"  {key}: {value}")
        
        return True
        
    except Exception as e:
        print(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("=" * 60)
    print("OpenVLA-7B æµ‹è¯•å¥—ä»¶")
    print("=" * 60)
    print()
    
    # æ£€æŸ¥ CUDA æ˜¯å¦å¯ç”¨
    if not torch.cuda.is_available():
        print("âš ï¸  è­¦å‘Š: CUDA ä¸å¯ç”¨ï¼Œæµ‹è¯•å¯èƒ½å¤±è´¥")
        print(f"   PyTorch ç‰ˆæœ¬: {torch.__version__}")
        return
    
    print(f"âœ… CUDA å¯ç”¨")
    print(f"   è®¾å¤‡æ•°é‡: {torch.cuda.device_count()}")
    print(f"   å½“å‰è®¾å¤‡: {torch.cuda.current_device()}")
    print(f"   è®¾å¤‡åç§°: {torch.cuda.get_device_name()}")
    print(f"   æ˜¾å­˜æ€»é‡: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    print()
    
    # è¿è¡Œæµ‹è¯•
    results = {
        "model_loading": False,
        "image_processing": False,
        "inference": False,
        "config": False,
    }
    
    # æµ‹è¯• 1: æ¨¡å‹åŠ è½½
    policy = test_model_loading()
    if policy is not None:
        results["model_loading"] = True
        
        # æµ‹è¯• 2: å›¾åƒå¤„ç†
        batch = test_image_processing(policy)
        if batch is not None:
            results["image_processing"] = True
            
            # æµ‹è¯• 3: æ¨ç†
            actions = test_inference(policy, batch)
            if actions is not None:
                results["inference"] = True
    
    # æµ‹è¯• 4: é…ç½®æ–‡ä»¶
    if test_config():
        results["config"] = True
    
    # æ€»ç»“
    print("\n" + "=" * 60)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 60)
    
    for test_name, passed in results.items():
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        print(f"{test_name:20s}: {status}")
    
    all_passed = all(results.values())
    print()
    if all_passed:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼OpenVLA-7B å·²æ­£ç¡®é…ç½®ã€‚")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚")
    
    return all_passed


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
